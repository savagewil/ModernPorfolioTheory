{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## IMPORT NECESSARY LIBRARIES \n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import autograd.numpy as npg\n",
    "from autograd import grad, elementwise_grad as e_grad\n",
    "import time\n",
    "from datetime import datetime,date\n",
    "import math\n",
    "import requests\n",
    "from bokeh.io import output_file, show\n",
    "from bokeh.models import ColumnDataSource, CrosshairTool, HoverTool, NumeralTickFormatter\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.layouts import column, row, gridplot, layout\n",
    "from bokeh.transform import cumsum\n",
    "import bankroll\n",
    "import glob\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamOptim():\n",
    "    def __init__(self, eta=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.m_dw, self.v_dw = 0, 0\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.eta = eta\n",
    "    def update(self, t, w, dw):\n",
    "        ## dw, db are from current minibatch\n",
    "        ## momentum beta 1\n",
    "        # *** weights *** #\n",
    "        self.m_dw = self.beta1*self.m_dw + (1-self.beta1)*dw\n",
    "\n",
    "        ## rms beta 2\n",
    "        # *** weights *** #\n",
    "        self.v_dw = self.beta2*self.v_dw + (1-self.beta2)*(dw**2)\n",
    "\n",
    "        ## bias correction\n",
    "        m_dw_corr = self.m_dw/(1-(self.beta1**t)+self.epsilon)\n",
    "        v_dw_corr = self.v_dw/(1-(self.beta2**t)+self.epsilon)\n",
    "\n",
    "        ## update weights and biases\n",
    "        w = w - self.eta*(m_dw_corr/(np.sqrt(v_dw_corr)+self.epsilon))\n",
    "        return w\n",
    "    def update_v(self, t, w, dw):\n",
    "        ## dw, db are from current minibatch\n",
    "        ## momentum beta 1\n",
    "        # *** weights *** #\n",
    "        self.m_dw = np.add(np.multiply(self.beta1,self.m_dw),np.multiply((1-self.beta1), dw))\n",
    "\n",
    "        ## rms beta 2\n",
    "        # *** weights *** #\n",
    "        self.v_dw = np.add(np.multiply(self.beta2, self.v_dw), np.multiply((1-self.beta2),np.square(dw)))\n",
    "\n",
    "        ## bias correction\n",
    "        m_dw_corr = np.divide(self.m_dw,np.subtract((1+self.epsilon),np.power(self.beta1,t)))\n",
    "        v_dw_corr = np.divide(self.v_dw,np.subtract((1+self.epsilon),np.power(self.beta2,t)))\n",
    "\n",
    "        ## update weights and biases\n",
    "        w = np.subtract(w, np.multiply(self.eta, np.divide(m_dw_corr, np.add(np.sqrt(v_dw_corr), self.epsilon))))\n",
    "        return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def norm(x):\n",
    "    x = x - np.min(x)\n",
    "    x = x / np.sum(x)\n",
    "    return x\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def tanh_refined(x):\n",
    "    return (np.tanh(x)+1.0)/2.0\n",
    "\n",
    "def get_weights(wts):\n",
    "#     wts = sigmoid(wts)\n",
    "    wts = npg.maximum(0.0, wts)\n",
    "    if npg.sum(wts) != 0:\n",
    "        wts = wts/npg.sum(wts)\n",
    "    return wts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_weights_v(wts):\n",
    "#     wts = sigmoid(wts)\n",
    "    wts = npg.maximum(0.0, wts)\n",
    "    if npg.sum(wts) != 0:\n",
    "        wts = wts/npg.sum(wts, axis=1, keepdims=True)\n",
    "    return wts\n",
    "\n",
    "def get_return_v(wts):\n",
    "    wts = get_weights_v(wts)\n",
    "    port_ret = npg.sum(log_ret_mean * wts, axis=1)\n",
    "    port_ret = npg.power((port_ret + 1), 252) - 1\n",
    "    return port_ret\n",
    "    \n",
    "def get_risk_v(wts):\n",
    "    wts = get_weights_v(wts)\n",
    "    port_sd = npg.sqrt(npg.sum(wts * npg.dot(wts, cov_mat.T), axis=1))\n",
    "    return port_sd\n",
    "\n",
    "def get_sharpe_v(wts):\n",
    "    port_ret = get_return_v(wts)\n",
    "    port_sd = get_risk_v(wts)\n",
    "    sr = port_ret / port_sd\n",
    "    return sr\n",
    "\n",
    "def get_loss_v(risk_target=None, return_target=None):\n",
    "    weight = 1.0e3\n",
    "    if risk_target is not None and return_target is not None:\n",
    "        return lambda x: ((npg.square(risk_target - get_risk_v(x)))+\n",
    "                          (npg.square(return_target - get_return_v(x))))\n",
    "    elif risk_target is not None:\n",
    "        return lambda x: ((weight*npg.square(risk_target - get_risk_v(x))) - get_return_v(x))/(weight + 1.0)\n",
    "    elif return_target is not None:\n",
    "        return lambda x: ((weight*npg.square(return_target - get_return_v(x))) - get_sharpe_v(x))/(weight + 1.0)\n",
    "    return lambda x: npg.negative(get_sharpe_v(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wts = np.random.random((5,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = get_loss_v()\n",
    "loss(wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def filterTickers(ticks, tick_allowed):\n",
    "    return [tick for tick, allowed in zip(ticks, tick_allowed) if allowed]\n",
    "\n",
    "def get_return(wts):\n",
    "    wts = npg.array(filterTickers(wts, tick_allowed))\n",
    "    wts = get_weights(wts)\n",
    "    port_ret = npg.sum(log_ret_mean * wts)\n",
    "    port_ret = (port_ret + 1) ** 252 - 1\n",
    "    return port_ret\n",
    "    \n",
    "def get_risk(wts):\n",
    "#     print(wts)\n",
    "    wts = npg.array(filterTickers(wts, tick_allowed))\n",
    "#     print(wts)\n",
    "    wts = get_weights(wts)\n",
    "    port_sd = npg.sqrt(npg.dot(wts.T, npg.dot(cov_mat, wts)))\n",
    "    return port_sd\n",
    "\n",
    "def get_sharpe(wts):\n",
    "    port_ret = get_return(wts)\n",
    "    port_sd = get_risk(wts)\n",
    "    sr = port_ret / port_sd\n",
    "    return sr\n",
    "\n",
    "def get_loss(risk_target=None, return_target=None):\n",
    "    weight = 1.0e3\n",
    "    if risk_target is not None and return_target is not None:\n",
    "        return lambda x: ((npg.square(risk_target - get_risk(x)))+\n",
    "                          (npg.square(return_target - get_return(x))))\n",
    "    elif risk_target is not None:\n",
    "        return lambda x: ((weight*npg.square(risk_target - get_risk(x))) - get_return(x))/(weight + 1.0)\n",
    "    elif return_target is not None:\n",
    "        return lambda x: ((weight*npg.square(return_target - get_return(x))) - get_sharpe(x))/(weight + 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def get_gd_weights(weights_size, loss_fun, batch, iterations, LR, scorings, initial=None):\n",
    "    # Optimize weights using gradient descent.\n",
    "    if initial is not None:\n",
    "        best_weights = get_weights(initial) + np.random.uniform(size=weights_size)\n",
    "    else:\n",
    "        best_weights = np.random.uniform(size=weights_size)\n",
    "    training_gradient_fun = elementwise_grad(loss_fun)\n",
    "    scores = np.zeros((len(scorings), batch, iterations))\n",
    "    for b in range(batch):\n",
    "        if initial is not None:\n",
    "            wts = get_weights(initial) + np.random.uniform(size=weights_size)\n",
    "        else:\n",
    "            wts = np.random.uniform(size=weights_size)\n",
    "#         wts = np.random.uniform(size=weights_size)\n",
    "        for i in range(iterations):\n",
    "            wts = wts - training_gradient_fun(wts) * LR\n",
    "            for s in range(len(scorings)):\n",
    "                print(scores[s, b, i].shape)\n",
    "                print(scorings[s](wts).shape)\n",
    "                scores[s, b, i] = scorings[s](wts)\n",
    "        if loss_fun(wts) < loss_fun(best_weights):\n",
    "            best_weights = wts\n",
    "    return best_weights, scores\n",
    "\n",
    "def get_gd_weights_adam(weights_size, loss_fun, batch, iterations, LR, scorings, initial=None):\n",
    "    adam = AdamOptim(eta=LR)\n",
    "    # Optimize weights using gradient descent.\n",
    "    if initial is not None:\n",
    "        best_weights = get_weights(initial) + np.random.uniform(size=weights_size)\n",
    "    else:\n",
    "        best_weights = np.random.uniform(size=weights_size)\n",
    "    training_gradient_fun = grad(loss_fun)\n",
    "    scores = np.zeros((len(scorings), batch, iterations))\n",
    "    for b in range(batch):\n",
    "        if initial is not None:\n",
    "            wts = get_weights(initial) + np.random.uniform(size=weights_size)\n",
    "        else:\n",
    "            wts = np.random.uniform(size=weights_size)*4\n",
    "#         wts = np.random.uniform(size=weights_size)\n",
    "        for i in range(iterations):\n",
    "            dw = training_gradient_fun(wts)\n",
    "            wts = adam.update(i+1,wts, dw)\n",
    "            for s in range(len(scorings)):\n",
    "#                 print(scorings[s](wts))\n",
    "                scores[s, b, i] = scorings[s](wts)\n",
    "        if loss_fun(wts) < loss_fun(best_weights):\n",
    "            best_weights = wts\n",
    "    return best_weights, scores\n",
    "\n",
    "\n",
    "\n",
    "def get_gd_weights_adam_batched(weights_size, loss_fun_v, batch, iterations, LR, scorings, initial=None):\n",
    "    adam = AdamOptim(eta=LR)\n",
    "    training_gradient_fun_v = e_grad(loss_fun_v)\n",
    "    scores = np.zeros((len(scorings), batch, iterations))\n",
    "    wts = np.random.uniform(size=(batch, weights_size))*4\n",
    "    print(wts.shape)\n",
    "    for i in range(iterations):\n",
    "        dw = training_gradient_fun_v(wts)\n",
    "        wts = adam.update(i+1,wts, dw)\n",
    "        for s in range(len(scorings)):\n",
    "            scores[s, :, i] = scorings[s](wts)\n",
    "    best_weights = wts[np.argmin(loss_fun_v(wts)),:]\n",
    "    return best_weights, scores\n",
    "\n",
    "def get_gd_weights_adam_batched_exp(weights_size, loss_fun_v, batch, iterations, LR, scorings, initial=None):\n",
    "    adam = AdamOptim(eta=LR)\n",
    "    training_gradient_fun_v = e_grad(loss_fun_v)\n",
    "    scores = np.zeros((len(scorings), batch, iterations))\n",
    "    wts = np.random.uniform(size=(batch, weights_size))*4\n",
    "    print(wts.shape)\n",
    "    for i in range(iterations):\n",
    "        dw = training_gradient_fun_v(wts)\n",
    "        wts = adam.update_v(i+1,wts, dw)\n",
    "        for s in range(len(scorings)):\n",
    "            scores[s, :, i] = scorings[s](wts)\n",
    "    best_weights = wts[np.argmin(loss_fun_v(wts)),:]\n",
    "    return best_weights, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_profiles = 5000\n",
    "inflation_rate = 0.06\n",
    "\n",
    "start_date = '01/06/2000'\n",
    "end_date = '01/06/2021'\n",
    "\n",
    "plt.rnparams['figure.figsize'] = [15, 5]\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Ticks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ticks = [\n",
    "    \"FITLX\",\n",
    "    \"FNIDX\",\n",
    "    \"FNDSX\",\n",
    "    \"SUSA\",\n",
    "    \"IQSU\", \n",
    "    \"USSG\",\n",
    "    \"SUSB\",\n",
    "    \"SNPE\",\n",
    "    \"SUSL\",\n",
    "    \"EAGG\",\n",
    "    \"WSBFX\",\n",
    "    \"NEXTX\",\n",
    "    \"CASH\",\n",
    "]\n",
    "tick_names = [\n",
    "    \"Fidelity® U.S. Sustainability Index Fund\",\n",
    "    \"Fidelity® International Sustainability Index Fund\",\n",
    "    \"Fidelity® Sustainability Bond Index Fund\",\n",
    "    \"iShares MSCI USA ESG Select ETF\", \n",
    "    \"IQ Candriam ESG US Equity ETF\", \n",
    "    \"Xtrackers MSCI USA ESG Leaders Eq ETF\",\n",
    "    \"iShares ESG 1-5 Year USD Corp Bd ETF\",\n",
    "    \"Xtrackers S&P 500 ESG ETF\",\n",
    "    \"iShares ESG MSCI USA Leaders ETF\",\n",
    "    \"iShares ESG Aware U.S. Aggregate Bond ETF\",\n",
    "    \"Boston Trust Walden Balanced Fund\", \n",
    "    \"Shelton Green Alpha Fund\",\n",
    "    \"Cash\",\n",
    "]\n",
    "tick_allowed = [\n",
    "    True,\n",
    "    True,\n",
    "    True,\n",
    "    True,\n",
    "    True, \n",
    "    True,\n",
    "    True,\n",
    "    True,\n",
    "    True,\n",
    "    True,\n",
    "    False,\n",
    "    False,\n",
    "    False,\n",
    "]\n",
    "ticks_filtered = filterTickers(ticks, tick_allowed)\n",
    "tick_name_filtered = filterTickers(tick_names, tick_allowed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./configs/profiles\"\n",
    "FILE = open(path, 'r')\n",
    "profiles = FILE.readlines()\n",
    "FILE.close()\n",
    "profiles = [profile.strip() for profile in profiles]\n",
    "profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles_names = [\n",
    "    \"Roth IRA\",\n",
    "    \"Investment Account\",\n",
    "    \"Amazon \",\n",
    "    \"Cash Account\",\n",
    "]\n",
    "profiles_targets = [\n",
    "    get_loss(risk_target=0.1),\n",
    "    None,\n",
    "    get_loss(),\n",
    "    None\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = \"./configs/apikey\"\n",
    "FILE = open(path, 'r')\n",
    "api_key = FILE.readline()\n",
    "FILE.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## MAIN BODY \n",
    "\n",
    "# Download historical data\n",
    "start = int(time.mktime(datetime.strptime(start_date, \"%d/%m/%Y\").timetuple()))\n",
    "end = int(time.mktime(datetime.strptime(end_date, \"%d/%m/%Y\").timetuple()))\n",
    "\n",
    "data_dict = dict()\n",
    "i = 0\n",
    "\n",
    "for stock in ticks_filtered:\n",
    "    if i != 0 and i % 10 == 0:\n",
    "        print(\"Sleeping for request limit\")\n",
    "        time.sleep(60)\n",
    "    \n",
    "#     querystring = {\"to\": end, \"symbol\": stock, \"from\": start, \"resolution\": 'D'}\n",
    "    querystring = f\"symbol={stock}&resolution=D&from={start}&to={end}&token={api_key}\"\n",
    "\n",
    "    try:\n",
    "        response = requests.request(\"GET\", url = (f\"https://finnhub.io/api/v1/stock/candle?{querystring}\"))\n",
    "        print(f\"https://finnhub.io/api/v1/stock/candle?{querystring}\")\n",
    "        \n",
    "        data = response.json()\n",
    "        df = pd.DataFrame.from_dict(data)\n",
    "        df = df.drop(columns=['s', 'h', 'l', 'o', 'v'])\n",
    "\n",
    "        # Output time zone: Universal Time Coordinated\n",
    "        df['time'] = [datetime.utcfromtimestamp(x).strftime('%Y-%m-%d %H:%M:%S') for x in df.t.values]\n",
    "\n",
    "    except:\n",
    "        raise ValueError(\"No data found for \" + stock)\n",
    "\n",
    "    i += 1\n",
    "\n",
    "    # The download limit is 10 requests per minute\n",
    "            \n",
    "    data_dict[stock] = df\n",
    "\n",
    "data_to_concat = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# PRE PROCESS DATA TO A FRIENDLY FORMAT\n",
    "for key in data_dict:\n",
    "    data_dict[key] = data_dict[key].rename(columns={\"c\": key})\n",
    "    data_to_concat.append(data_dict[key])\n",
    "\n",
    "price_data = pd.concat(data_to_concat, axis=1, join='inner')\n",
    "price_data = price_data.loc[:,~price_data.columns.duplicated()].drop(columns='t').set_index('time')\n",
    "\n",
    "log_ret = np.log(price_data/price_data.shift(1))\n",
    "\n",
    "cov_mat = log_ret.cov() * 252\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Profile Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_of_files = glob.glob('.\\profiles\\*.csv')\n",
    "latest_file = max(list_of_files, key=os.path.getctime)\n",
    "df = pd.read_csv(latest_file)\n",
    "df = df[~df.Description.isna()]\n",
    "df = df[['Account Number', 'Symbol', 'Current Value']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_makeup = pd.DataFrame(data = np.zeros((len(profiles), len(ticks))), \n",
    "                              index=profiles, \n",
    "                              columns=ticks)\n",
    "profile_makeup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row_ in df.iterrows():\n",
    "    if row_[1][\"Symbol\"] in ticks and  row_[1][\"Account Number\"] in profiles:\n",
    "        profile_makeup.loc[row_[1][\"Account Number\"], row_[1][\"Symbol\"]] = float(row_[1][\"Current Value\"][1:])\n",
    "    elif row_[1][\"Account Number\"] in profiles:\n",
    "        profile_makeup.loc[row_[1][\"Account Number\"], \"CASH\"] = float(row_[1][\"Current Value\"][1:])\n",
    "profile_makeup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "source": [
    "## SIMULATE x PORTFOLIOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_wts = np.zeros((num_profiles, len(ticks_filtered)))\n",
    "port_returns = np.zeros((num_profiles))\n",
    "port_risk = np.ones((num_profiles))\n",
    "sharpe_ratio = np.zeros((num_profiles))\n",
    "\n",
    "for i in range(num_profiles):\n",
    "    if i % (num_profiles//10)==0:\n",
    "            print(i, f\"{(100 * i / num_profiles)}%\")\n",
    "    \n",
    "    \n",
    "    # Portfolio weights\n",
    "    wts = np.random.uniform(size=len(ticks_filtered))\n",
    "    \n",
    "    wts = wts / np.sum(wts)\n",
    "#     print(w?ts)\n",
    "    all_wts[i, :] = wts\n",
    "\n",
    "    # Portfolio Return\n",
    "    port_ret = np.sum(log_ret.mean() * wts)\n",
    "    port_ret = (port_ret + 1) ** 252 - 1\n",
    "    port_returns[i] = port_ret\n",
    "\n",
    "    # Portfolio Risk\n",
    "    port_sd = np.sqrt(np.dot(wts.T, np.dot(cov_mat, wts)))\n",
    "    port_risk[i] = port_sd\n",
    "\n",
    "    # Portfolio Sharpe Ratio, assuming 0% Risk Free Rate\n",
    "    sr = port_ret / port_sd\n",
    "    sharpe_ratio[i] = sr\n",
    "\n",
    "    # Save portfolios of interest (min var, max return and max SR)\n",
    "    if sr >= max(sharpe_ratio[0:-1]):\n",
    "        max_sr_ret = port_ret\n",
    "        max_sr_risk = port_sd\n",
    "        max_sr_w = wts\n",
    "        max_sr = sr\n",
    "\n",
    "    if port_ret >= max(port_returns[0:-1]):\n",
    "        max_ret_ret = port_ret\n",
    "        max_ret_risk = port_sd\n",
    "        max_ret_w = wts\n",
    "\n",
    "    if port_sd <= min(port_risk[0:-1]):\n",
    "        min_var_ret = port_ret\n",
    "        min_var_risk = port_sd\n",
    "        min_var_w = wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cov_mat = np.array(cov_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_ret_mean = np.array(log_ret.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get Max Sharpe\n",
    "batch = 100\n",
    "iterations = 100\n",
    "LR=0.1\n",
    "\n",
    "loss_fun = lambda x: -get_sharpe(x)\n",
    "loss_fun_v = get_loss_v()\n",
    "\n",
    "start_time = time.time()\n",
    "best_sharpe_weights, loss = get_gd_weights_adam_batched_exp(len(ticks_filtered), loss_fun_v, batch, iterations, LR, [get_sharpe_v])\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "loss = loss[0,:,:]\n",
    "plt.plot(loss.T)\n",
    "plt.show()\n",
    "print(get_sharpe(best_sharpe_weights))\n",
    "\n",
    "start_time = time.time()\n",
    "best_sharpe_weights, loss = get_gd_weights_adam(len(ticks_filtered), loss_fun, batch, iterations, LR, [get_sharpe])\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "loss = loss[0,:,:]\n",
    "plt.plot(loss.T)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Get Min Risk\n",
    "batch = 1000\n",
    "iterations = 1000\n",
    "LR=0.01\n",
    "start_time = time.time()\n",
    "least_risk_weights, loss = get_gd_weights_adam_batched(len(ticks_filtered), get_risk_v, batch, iterations, LR, [get_risk_v])\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "loss = loss[0,:,:]\n",
    "\n",
    "plt.plot(loss.T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get Max Return\n",
    "batch = 1000\n",
    "iterations = 200\n",
    "LR=0.1\n",
    "\n",
    "start_time = time.time()\n",
    "most_return_weights, loss = get_gd_weights_adam_batched(len(ticks_filtered), \n",
    "                                           lambda x: npg.negative(get_return_v(x)),\n",
    "                                           batch, iterations,\n",
    "                                           LR, [get_return_v])\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "# start_time = time.time()\n",
    "# most_return_weights, loss = get_gd_weights_adam(len(ticks_filtered), \n",
    "#                                            lambda x: -get_return(x),\n",
    "#                                            batch, iterations,\n",
    "#                                            LR, [get_risk])\n",
    "# print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "loss = loss[0,:,:]\n",
    "\n",
    "plt.plot(loss.T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get List of Max Returns for Spread of risks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_min = 2.0/100.0\n",
    "risk_max = 25.0/100.0\n",
    "risks_count = 24\n",
    "risks = np.linspace(risk_min, risk_max, risks_count, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 100\n",
    "i = 500\n",
    "lr = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "50*np.abs(0.07 - risks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch = [b]*risks_count\n",
    "iterations = [i]*risks_count\n",
    "LR=np.array([lr]*risks_count)\n",
    "# LR = LR*(1.0 + 50*np.abs(0.07 - risks))\n",
    "# LR[-3]*=10\n",
    "\n",
    "best_weights_range = np.random.uniform(size=(risks_count, len(ticks_filtered))) \n",
    "for r in range(risks_count):\n",
    "    best_weights_range[r, :], scoring = get_gd_weights_adam(len(ticks_filtered),\n",
    "                                                  get_loss(risk_target=risks[r]),\n",
    "                                                  batch[r],\n",
    "                                                  iterations[r],\n",
    "                                                  LR[r],\n",
    "                                                  [get_risk,get_return])\n",
    "    risk,returns = scoring[0,:,:],scoring[1,:,:]\n",
    "    fig, ax = plt.subplots(1,2)\n",
    "    ax[0].plot(risk.T)\n",
    "    ax[0].set_title('Risk %f' % risks[r])\n",
    "    ax[1].plot(returns.T)\n",
    "    ax[1].set_title('Return')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profile to match inflation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch = 20\n",
    "iterations = 400\n",
    "LR=1.0\n",
    "\n",
    "inflation_weights, scoring = get_gd_weights_adam(len(ticks_filtered), \n",
    "                                           get_loss(return_target=inflation_rate),\n",
    "                                           batch, iterations,\n",
    "                                           LR, [get_risk,get_return])\n",
    "risk,returns = scoring[0,:,:],scoring[1,:,:]\n",
    "fig, ax = plt.subplots(1,2)\n",
    "ax[0].plot(risk.T)\n",
    "ax[0].set_title('Risk')\n",
    "ax[1].plot(returns.T)\n",
    "ax[1].set_title('Return %f'%inflation_rate)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "b = 1000\n",
    "i = 1000\n",
    "lr = 0.001\n",
    "batch = [b]*len(profiles)\n",
    "iterations = [i]*len(profiles)\n",
    "LR=[lr]*len(profiles)\n",
    "\n",
    "# batch[2] = 10\n",
    "# iterations[2] = 200\n",
    "# LR[2]=0.1\n",
    "target_weights = []\n",
    "\n",
    "for i in range(len(profiles)):\n",
    "    if profiles_targets[i] is not None:\n",
    "        wts = profile_makeup.loc[profiles[i]].to_numpy()[:-1]\n",
    "        print(i)\n",
    "        target_weights_, scoring = get_gd_weights_adam(len(ticks_filtered),\n",
    "                                                    profiles_targets[i],\n",
    "                                                    batch[i], iterations[i],\n",
    "                                                    LR[i], [get_risk,get_return],\n",
    "                                                    initial = filterTickers(wts, tick_allowed))\n",
    "        target_weights.append(target_weights_)\n",
    "        risk,returns = scoring[0,:,:],scoring[1,:,:]\n",
    "        fig, ax = plt.subplots(1,2)\n",
    "        ax[0].plot(risk.T)\n",
    "        ax[0].set_title('Risk')\n",
    "        ax[1].plot(returns.T)\n",
    "        ax[1].set_title('Return')\n",
    "        plt.show()\n",
    "    else:\n",
    "        \n",
    "        wts = profile_makeup.loc[profiles[i]].to_numpy()\n",
    "        target_weights.append(wts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "color_list = [\"#677737\", \"#1d27f1\", \"#e687de\", \n",
    "              \"#764484\", \"#edac3a\", \"#e2ca50\",\n",
    "              \"#c80727\",\"#7ea0fb\", \"#4bddac\", \n",
    "              \"#4c6ab5\", \"#16287c\",\"#e9223b\",\n",
    "              \"#db3f9e\", \"#38f716\",\"#d9510a\",\n",
    "             ]\n",
    "color_list_accounts = [\"#FCBD1F\", \"#B24157\", \"#ABC962\", \n",
    "              \"#1DD4AF\", \"#edac3a\", \"#e2ca50\",\n",
    "              \"#c80727\",\"#7ea0fb\", \"#4bddac\", \n",
    "              \"#4c6ab5\", \"#16287c\",\"#e9223b\",\n",
    "              \"#db3f9e\", \"#38f716\",\"#d9510a\",\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## PLOTS\n",
    "p = figure(\n",
    "    sizing_mode = \"stretch_both\", \n",
    "#     plot_width=10000,\n",
    "    title=\"Efficient frontier. Simulations: \" + str(num_profiles),\n",
    "    tools='box_zoom,wheel_zoom,reset', \n",
    "    toolbar_location='above',\n",
    ")\n",
    "p.add_tools(CrosshairTool(line_alpha=1, line_color='lightgray', line_width=1))\n",
    "p.add_tools(HoverTool(tooltips=None))\n",
    "source = ColumnDataSource(data=dict(risk=port_risk, profit=port_returns))\n",
    "p.circle(x='risk', y='profit', source=source, line_alpha=0, hover_color='navy', alpha=0.4, hover_alpha=1, size=8)\n",
    "\n",
    "for i in range(len(ticks_filtered)):\n",
    "    wts = -0 * np.ones(len(ticks_filtered))\n",
    "    wts[i]=1.0\n",
    "    p.circle(get_risk(wts), get_return(wts), color=color_list[i], legend_label=ticks[i], size=8, alpha=0.4)\n",
    "    \n",
    "p.circle(get_risk(least_risk_weights), get_return(least_risk_weights), color='tomato', \n",
    "         legend_label='Portfolio with min risk', size=12)\n",
    "p.circle(get_risk(inflation_weights), get_return(inflation_weights), color='limeGreen',\n",
    "         legend_label='Portfolio matching Inflation', size=12)\n",
    "p.circle(get_risk(most_return_weights), get_return(most_return_weights), color='firebrick',\n",
    "         legend_label='Portfolio with max return', size=12)\n",
    "p.circle(get_risk(best_sharpe_weights), get_return(best_sharpe_weights), color='Green', \n",
    "         legend_label='Portfolio with max Sharpe', size=12)\n",
    "\n",
    "for r in range(risks_count):\n",
    "    wts = best_weights_range[r,:]\n",
    "    p.circle(get_risk(wts), get_return(wts), color=\"teal\", legend_label=tick_names[i], size=9)\n",
    "p.legend.location = \"bottom_right\"\n",
    "\n",
    "p.xaxis.axis_label = 'Volatility, or risk (standard deviation)'\n",
    "p.yaxis.axis_label = 'Annual return'\n",
    "p.xaxis[0].formatter = NumeralTickFormatter(format=\"0.0%\")\n",
    "p.yaxis[0].formatter = NumeralTickFormatter(format=\"0.0%\")\n",
    "\n",
    "# Portfolio composition. Min variance, max SR, max return\n",
    "def plot_portfolio_composition(ticks, weights, plot_name):\n",
    "    print(ticks)\n",
    "    x = dict()\n",
    "    c = dict()\n",
    "    for i in range(len(ticks)):\n",
    "        if weights[i] >= 1.0:\n",
    "            x[ticks[i]] = weights[i]\n",
    "            c[ticks[i]] = color_list[i]\n",
    "            x[ticks[i]+\" \"] = weights[i]\n",
    "            c[ticks[i]+\" \"] = color_list[i]\n",
    "        elif weights[i] > 0.0:\n",
    "            x[ticks[i]] = weights[i]\n",
    "            c[ticks[i]] = color_list[i]\n",
    "\n",
    "    plot_data = pd.Series(x).reset_index(name='value').rename(columns={'index': 'stock'})\n",
    "    plot_data['angle'] = plot_data['value'] / plot_data['value'].sum() * 2 * math.pi\n",
    "    \n",
    "    plot_data['color'] = c.values()\n",
    "    \n",
    "    print(plot_data)\n",
    "    print()\n",
    "    p = figure(width=50, height=50, title=plot_name, toolbar_location=None,sizing_mode = \"scale_height\",\n",
    "                      tools=\"hover\", tooltips=\"@stock: @value{%0.1f}\", x_range=(-0.5,0.5))\n",
    "    p.wedge(x=0, y=1, radius=0.4, start_angle=cumsum('angle', include_zero=True), end_angle=cumsum('angle'),\n",
    "                   line_color=\"white\", color='color', source=plot_data)\n",
    "    p.axis.axis_label = None\n",
    "    p.axis.visible = False\n",
    "    p.grid.grid_line_color = None\n",
    "    p.outline_line_color = None\n",
    "\n",
    "    return p\n",
    "\n",
    "# p_minvar = plot_portfolio_composition(ticks, get_weights(least_risk_weights), \"Portfolio with Min Risk\")\n",
    "# p_maxsr = plot_portfolio_composition(ticks, get_weights(inflation_weights), \"Portfolio matching Inflation\")\n",
    "# p_maxGD = plot_portfolio_composition(ticks, get_weights(best_sharpe_weights), \"Portfolio with max Sharpe GD\")\n",
    "# p_maxret = plot_portfolio_composition(ticks, get_weights(most_return_weights), \"Portfolio with max return\")\n",
    "\n",
    "\n",
    "fidelity_pies = []\n",
    "\n",
    "for i in range(len(profiles)):\n",
    "    wts = get_weights(profile_makeup.loc[profiles[i]].to_numpy())\n",
    "    fidelity_pies.append(plot_portfolio_composition(ticks,\n",
    "                                                    wts,\n",
    "                                                    profiles_names[i]))\n",
    "    if wts[-1] == 0.0:\n",
    "        p.circle(get_risk(wts[:-1]), get_return(wts[:-1]), color=color_list_accounts[i], \n",
    "             legend_label=profiles_names[i], size=12)\n",
    "fidelity_targets = []\n",
    "\n",
    "for i in range(len(target_weights)):\n",
    "    fidelity_targets.append(plot_portfolio_composition((ticks if len(target_weights[i]) > len(ticks_filtered) else ticks_filtered),\n",
    "                                                    get_weights(target_weights[i]),\n",
    "                                                    profiles_names[i] + \" Target\"))\n",
    "    if len(target_weights[i]) == len(ticks_filtered):\n",
    "        p.circle(get_risk(get_weights(target_weights[i])), get_return(get_weights(target_weights[i])),\n",
    "                 color=color_list_accounts[i], alpha=0.6,\n",
    "                 legend_label=profiles_names[i] + \" Target\", size=12)\n",
    "\n",
    "# Create dashboard and open new window to show results\n",
    "layout_ = row([ \n",
    "        column([\n",
    "            p\n",
    "        ], sizing_mode = \"stretch_both\"),\n",
    "        column(fidelity_pies,\n",
    "               sizing_mode = \"stretch_height\"), \n",
    "        column(fidelity_targets\n",
    "               , sizing_mode = \"stretch_height\"),\n",
    "    ],width = 1500, sizing_mode = \"stretch_height\")\n",
    "    \n",
    "show(layout_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

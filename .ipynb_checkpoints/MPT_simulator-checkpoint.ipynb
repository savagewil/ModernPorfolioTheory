{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## IMPORT NECESSARY LIBRARIES \n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import autograd.numpy as npg\n",
    "from autograd import grad, elementwise_grad as e_grad\n",
    "import time\n",
    "from datetime import datetime,date\n",
    "import math\n",
    "import requests\n",
    "from bokeh.io import output_file, show\n",
    "from bokeh.models import ColumnDataSource, CrosshairTool, HoverTool, NumeralTickFormatter, Span\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.layouts import column, row, gridplot, layout\n",
    "from bokeh.transform import cumsum\n",
    "from bokeh.colors import RGB\n",
    "import bankroll\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "import colorsys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamOptim():\n",
    "    def __init__(self, eta=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.m_dw, self.v_dw = 0, 0\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.eta = eta\n",
    "    def update(self, t, w, dw):\n",
    "        self.m_dw = np.add(np.multiply(self.beta1,self.m_dw),np.multiply((1-self.beta1), dw))\n",
    "\n",
    "        self.v_dw = np.add(np.multiply(self.beta2, self.v_dw), np.multiply((1-self.beta2),np.square(dw)))\n",
    "\n",
    "        m_dw_corr = np.divide(self.m_dw,np.subtract((1+self.epsilon),np.power(self.beta1,t)))\n",
    "        v_dw_corr = np.divide(self.v_dw,np.subtract((1+self.epsilon),np.power(self.beta2,t)))\n",
    "\n",
    "        w = np.subtract(w, np.multiply(self.eta, np.divide(m_dw_corr, np.add(np.sqrt(v_dw_corr), self.epsilon))))\n",
    "        return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(x):\n",
    "    x = x - np.min(x)\n",
    "    x = x / np.sum(x)\n",
    "    return x\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def tanh_refined(x):\n",
    "    return (np.tanh(x)+1.0)/2.0\n",
    "\n",
    "def get_weights(wts):\n",
    "#     wts = sigmoid(wts)\n",
    "    wts = npg.maximum(0.0, wts)\n",
    "    if npg.sum(wts) != 0:\n",
    "        wts = wts/npg.sum(wts)\n",
    "    return wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights_v(wts):\n",
    "#     wts = sigmoid(wts)\n",
    "    wts = npg.maximum(0.0, wts)\n",
    "    if npg.sum(wts) != 0:\n",
    "        wts = wts/npg.sum(wts, axis=1, keepdims=True)\n",
    "    return wts\n",
    "\n",
    "def get_return_v(wts):\n",
    "    wts = get_weights_v(wts)\n",
    "    port_ret = npg.sum(log_ret_mean * wts, axis=1)\n",
    "    port_ret = npg.power((port_ret + 1), 252) - 1\n",
    "    return port_ret\n",
    "    \n",
    "def get_risk_v(wts):\n",
    "    wts = get_weights_v(wts)\n",
    "    port_sd = npg.sqrt(npg.sum(wts * npg.dot(wts, cov_mat.T), axis=1))\n",
    "    return port_sd\n",
    "\n",
    "def get_sharpe_v(wts):\n",
    "    port_ret = get_return_v(wts)\n",
    "    port_sd = get_risk_v(wts)\n",
    "    sr = port_ret / port_sd\n",
    "    return sr\n",
    "\n",
    "def get_loss_v(risk_target=None, return_target=None, sharpe_target=None):\n",
    "    weight = 1.0e3\n",
    "    if risk_target is not None and return_target is not None:\n",
    "        return lambda x: (weight*(npg.square(risk_target - get_risk_v(x)))+\n",
    "                          (npg.square(return_target - get_return_v(x))))/(weight + 1.0)\n",
    "    elif risk_target is not None and sharpe_target is not None:\n",
    "        return lambda x: ((weight*npg.square(risk_target - get_risk_v(x)))+\n",
    "                          (npg.square((sharpe_target - get_sharpe_v(x))/2.0)))/(weight + 1.0)\n",
    "    elif risk_target is not None:\n",
    "        return lambda x: ((weight*npg.square(risk_target - get_risk_v(x))) - get_return_v(x))/(weight + 1.0)\n",
    "    elif return_target is not None:\n",
    "        return lambda x: ((weight*npg.square(return_target - get_return_v(x))) + get_risk_v(x))/(weight + 1.0)\n",
    "    return lambda x: npg.negative(get_sharpe_v(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def filterTickers(ticks, tick_allowed):\n",
    "    return [tick for tick, allowed in zip(ticks, tick_allowed) if allowed]\n",
    "\n",
    "def get_return(wts):\n",
    "    wts = get_weights(wts)\n",
    "    port_ret = npg.sum(log_ret_mean * wts)\n",
    "    port_ret = (port_ret + 1) ** 252 - 1\n",
    "    return port_ret\n",
    "    \n",
    "def get_risk(wts):\n",
    "    wts = get_weights(wts)\n",
    "    port_sd = npg.sqrt(npg.dot(wts.T, npg.dot(cov_mat, wts)))\n",
    "    return port_sd\n",
    "\n",
    "def get_sharpe(wts):\n",
    "    port_ret = get_return(wts)\n",
    "    port_sd = get_risk(wts)\n",
    "    sr = port_ret / port_sd\n",
    "    return sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gd_weights(weights_size, loss_fun, batch, iterations, LR, scorings, initial=None):\n",
    "    # Optimize weights using gradient descent.\n",
    "    if initial is not None:\n",
    "        best_weights = get_weights(initial) + np.random.uniform(size=weights_size)\n",
    "    else:\n",
    "        best_weights = np.random.uniform(size=weights_size)\n",
    "    training_gradient_fun = grad(loss_fun)\n",
    "    scores = np.zeros((len(scorings), batch, iterations))\n",
    "    for b in range(batch):\n",
    "        if initial is not None:\n",
    "            wts = get_weights(initial) + np.random.uniform(size=weights_size)\n",
    "        else:\n",
    "            wts = np.random.uniform(size=weights_size)\n",
    "#         wts = np.random.uniform(size=weights_size)\n",
    "        for i in range(iterations):\n",
    "            wts = wts - training_gradient_fun(wts) * LR\n",
    "            for s in range(len(scorings)):\n",
    "                scores[s, b, i] = scorings[s](wts)\n",
    "        if loss_fun(wts) < loss_fun(best_weights):\n",
    "            best_weights = wts\n",
    "    return best_weights, scores\n",
    "\n",
    "def calculate_weights(weights_size, loss_fun_v, batch, iterations, LR, scorings, initial=None):\n",
    "    adam = AdamOptim(eta=LR)\n",
    "    training_gradient_fun_v = e_grad(loss_fun_v)\n",
    "    scores = np.zeros((len(scorings), batch, iterations))\n",
    "    wts = np.random.uniform(size=(batch, weights_size))*4\n",
    "    print(wts.shape)\n",
    "    for i in range(iterations):\n",
    "        dw = training_gradient_fun_v(wts)\n",
    "        wts = adam.update(i+1,wts, dw)\n",
    "        for s in range(len(scorings)):\n",
    "            scores[s, :, i] = scorings[s](wts)\n",
    "    best_weights = wts[np.argmin(loss_fun_v(wts)),:]\n",
    "    return best_weights, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_risk_vs_return(risk, returns, risk_title=\"Risk\", return_title=\"Return\"):\n",
    "    fig, ax = plt.subplots(1,2)\n",
    "    ax[0].plot(risk.T)\n",
    "    ax[0].set_title(risk_title)\n",
    "    ax[1].plot(returns.T)\n",
    "    ax[1].set_title(return_title)\n",
    "    \n",
    "    ax[0].set_ylabel('Risk')\n",
    "    ax[0].set_xlabel('Iteration')\n",
    "    ax[1].set_ylabel('Return')\n",
    "    ax[1].set_xlabel('Iteration')\n",
    "    plt.show()\n",
    "\n",
    "    # Portfolio composition. Min variance, max SR, max return\n",
    "def plot_portfolio_composition(ticks, weights, plot_name, color_list):\n",
    "    print(ticks)\n",
    "    x = dict()\n",
    "    c = dict()\n",
    "    for i in range(len(ticks)):\n",
    "        if weights[i] >= 1.0:\n",
    "            x[ticks[i]] = weights[i]\n",
    "            c[ticks[i]] = color_list[i]\n",
    "            x[ticks[i]+\" \"] = weights[i]\n",
    "            c[ticks[i]+\" \"] = color_list[i]\n",
    "        elif weights[i] > 0.0:\n",
    "            x[ticks[i]] = weights[i]\n",
    "            c[ticks[i]] = color_list[i]\n",
    "\n",
    "    plot_data = pd.Series(x).reset_index(name='value').rename(columns={'index': 'stock'})\n",
    "    plot_data['angle'] = plot_data['value'] / plot_data['value'].sum() * 2 * math.pi\n",
    "    \n",
    "    plot_data['color'] = c.values()\n",
    "    \n",
    "    print(plot_data)\n",
    "    print()\n",
    "    p = figure(width=50, height=50, title=plot_name, toolbar_location=None,sizing_mode = \"scale_height\",\n",
    "                      tools=\"hover\", tooltips=\"@stock: @value{%0.1f}\", x_range=(-0.5,0.5))\n",
    "    p.wedge(x=0, y=1, radius=0.4, start_angle=cumsum('angle', include_zero=True), end_angle=cumsum('angle'),\n",
    "                   line_color=\"white\", color='color', source=plot_data)\n",
    "    p.axis.axis_label = None\n",
    "    p.axis.visible = False\n",
    "    p.grid.grid_line_color = None\n",
    "    p.outline_line_color = None\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_profiles = 5000\n",
    "inflation_rate = 0.06\n",
    "\n",
    "start_date = '2000-06-01 00:00:00'\n",
    "end_date = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [15, 5]\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'2000-06-01 00:00:00'"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'2021-12-03 23:12:34'"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Ticks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ticks = [\n",
    "    \"FCOM\",\n",
    "    \"TRZBX\",\n",
    "    \"TILWX\",\n",
    "    \"FITLX\",\n",
    "    \"FNIDX\",\n",
    "    \"FNDSX\",\n",
    "    \"SUSA\",\n",
    "    \"IQSU\", \n",
    "    \"USSG\",\n",
    "    \"SUSB\",\n",
    "    \"SNPE\",\n",
    "    \"SUSL\",\n",
    "    \"EAGG\",\n",
    "    \"WSBFX\",\n",
    "    \"NEXTX\",\n",
    "    \"CASH\",\n",
    "]\n",
    "tick_names = [\n",
    "    \"Fidelity® MSCI Communication ServicesETF\",\n",
    "    \"T. Rowe Price Blue Chip Growth Z\",\n",
    "    \"TIAA-CREF Large-Cap Growth W\",\n",
    "    \"Fidelity® U.S. Sustainability Index Fund\",\n",
    "    \"Fidelity® International Sustainability Index Fund\",\n",
    "    \"Fidelity® Sustainability Bond Index Fund\",\n",
    "    \"iShares MSCI USA ESG Select ETF\", \n",
    "    \"IQ Candriam ESG US Equity ETF\", \n",
    "    \"Xtrackers MSCI USA ESG Leaders Eq ETF\",\n",
    "    \"iShares ESG 1-5 Year USD Corp Bd ETF\",\n",
    "    \"Xtrackers S&P 500 ESG ETF\",\n",
    "    \"iShares ESG MSCI USA Leaders ETF\",\n",
    "    \"iShares ESG Aware U.S. Aggregate Bond ETF\",\n",
    "    \"Boston Trust Walden Balanced Fund\", \n",
    "    \"Shelton Green Alpha Fund\",\n",
    "    \"Cash\",\n",
    "]\n",
    "\n",
    "tick_allowed = [\n",
    "    True,#\"FCOM\",\n",
    "    True,#\"TRZBX\",\n",
    "    True,#\"TILWX\",\n",
    "    True,#\"FITLX\",\n",
    "    True,#\"FNIDX\",\n",
    "    True,#\"FNDSX\",\n",
    "    True,#\"SUSA\",\n",
    "    True,#\"IQSU\", \n",
    "    True,#\"USSG\",\n",
    "    True,#\"SUSB\",\n",
    "    True,#\"SNPE\",\n",
    "    True,#\"SUSL\",\n",
    "    True,#\"EAGG\",\n",
    "    False,#\"WSBFX\",\n",
    "    False,#\"NEXTX\",\n",
    "    False,#\"CASH\",\n",
    "]\n",
    "ticks_filtered = filterTickers(ticks, tick_allowed)\n",
    "tick_name_filtered = filterTickers(tick_names, tick_allowed)\n",
    "assert len(ticks) == len(tick_allowed)\n",
    "assert len(ticks) == len(tick_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './configs/profiles'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-12-72caafb545af>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[0mpath\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m\"./configs/profiles\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0mFILE\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mopen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'r'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[0mprofiles\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mFILE\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreadlines\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0mFILE\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mclose\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[0mprofiles\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mprofile\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstrip\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mprofile\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mprofiles\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: './configs/profiles'"
     ]
    }
   ],
   "source": [
    "path = \"./configs/profiles\"\n",
    "FILE = open(path, 'r')\n",
    "profiles = FILE.readlines()\n",
    "FILE.close()\n",
    "profiles = [profile.strip() for profile in profiles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles_names = [\n",
    "    \"Roth IRA\",\n",
    "    \"Investment Account\",\n",
    "    \"Amazon \",\n",
    "    \"Cash Account\",\n",
    "]\n",
    "profiles_targets = [\n",
    "    get_loss_v(risk_target=0.1),\n",
    "    None,\n",
    "    get_loss_v(),\n",
    "    None\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = \"./configs/apikey\"\n",
    "FILE = open(path, 'r')\n",
    "api_key = FILE.readline()\n",
    "FILE.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## MAIN BODY \n",
    "\n",
    "# Download historical data\n",
    "start = int(time.mktime(datetime.strptime(start_date, \"%Y-%m-%d %H:%M:%S\").timetuple()))\n",
    "end = int(time.mktime(datetime.strptime(end_date, \"%Y-%m-%d %H:%M:%S\").timetuple()))\n",
    "\n",
    "data_dict = dict()\n",
    "i = 1\n",
    "\n",
    "for stock in ticks_filtered:\n",
    "    if i % 20 == 0:\n",
    "        print(\"Sleeping for request limit\")\n",
    "        time.sleep(60)\n",
    "    \n",
    "#     querystring = {\"to\": end, \"symbol\": stock, \"from\": start, \"resolution\": 'D'}\n",
    "    querystring = f\"symbol={stock}&resolution=D&from={start}&to={end}&token={api_key}\"\n",
    "\n",
    "    try:\n",
    "        response = requests.request(\"GET\", url = (f\"https://finnhub.io/api/v1/stock/candle?{querystring}\"))\n",
    "        print(f\"https://finnhub.io/api/v1/stock/candle?{querystring}\")\n",
    "        \n",
    "        data = response.json()\n",
    "        df = pd.DataFrame.from_dict(data)\n",
    "        df = df.drop(columns=['s', 'h', 'l', 'o', 'v'])\n",
    "\n",
    "        # Output time zone: Universal Time Coordinated\n",
    "        df['time'] = [datetime.utcfromtimestamp(x).strftime('%Y-%m-%d %H:%M:%S') for x in df.t.values]\n",
    "\n",
    "    except:\n",
    "        raise ValueError(\"No data found for \" + stock)\n",
    "\n",
    "    i += 1\n",
    "\n",
    "    # The download limit is 10 requests per minute\n",
    "            \n",
    "    data_dict[stock] = df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRE PROCESS DATA TO A FRIENDLY FORMAT\n",
    "data_to_concat = []\n",
    "data_dict_new = {}\n",
    "for key in data_dict:\n",
    "    data_dict_new[key] = data_dict[key].rename(columns={\"c\": key})\n",
    "    data_dict_new[key]['time'] = pd.to_datetime(data_dict_new[key]['time']).dt.date\n",
    "    data_dict_new[key] = data_dict_new[key].set_index(\"time\")\n",
    "    data_to_concat.append(data_dict_new[key])\n",
    "    \n",
    "price_data = pd.concat(data_to_concat, axis=1)\n",
    "price_data = price_data.loc[:,~price_data.columns.duplicated()].drop(columns='t')\n",
    "price_data = price_data.sort_index()\n",
    "\n",
    "log_ret = np.log(price_data/price_data.shift(1))\n",
    "\n",
    "cov_mat = np.array(log_ret.cov() * 252)\n",
    "log_ret_mean = np.array(log_ret.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Profile Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_of_files = glob.glob('.\\profiles\\*.csv')\n",
    "latest_file = max(list_of_files, key=os.path.getctime)\n",
    "df = pd.read_csv(latest_file)\n",
    "df = df[~df.Description.isna()]\n",
    "df = df[['Account Number', 'Symbol', 'Current Value']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_makeup = pd.DataFrame(data = np.zeros((len(profiles), len(ticks))), \n",
    "                              index=profiles, \n",
    "                              columns=ticks)\n",
    "profile_makeup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row_ in df.iterrows():\n",
    "    if row_[1][\"Symbol\"] in ticks and  row_[1][\"Account Number\"] in profiles:\n",
    "        profile_makeup.loc[row_[1][\"Account Number\"], row_[1][\"Symbol\"]] = float(row_[1][\"Current Value\"][1:])\n",
    "    elif row_[1][\"Account Number\"] in profiles:\n",
    "        profile_makeup.loc[row_[1][\"Account Number\"], \"CASH\"] = float(row_[1][\"Current Value\"][1:])\n",
    "profile_makeup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Limits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Max Sharpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch = 1000\n",
    "iterations = 1000\n",
    "LR=0.05\n",
    "\n",
    "start_time = time.time()\n",
    "best_sharpe_weights, scoring = calculate_weights(len(ticks_filtered),\n",
    "                                                        get_loss_v(), batch,\n",
    "                                                        iterations, LR,\n",
    "                                                        [get_risk_v,get_return_v])\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "plot_risk_vs_return(scoring[0,:,:],scoring[1,:,:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Risk Limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Get Min Risk\n",
    "batch = 1000\n",
    "iterations = 1000\n",
    "LR=1.0\n",
    "\n",
    "start_time = time.time()\n",
    "min_risk_weights, scoring = calculate_weights(len(ticks_filtered),\n",
    "                                                get_risk_v, batch,\n",
    "                                                iterations, LR, \n",
    "                                                [get_risk_v, get_return_v])\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# least_risk_weights, loss = get_gd_weights_adam(len(ticks_filtered), get_risk, batch, iterations, LR, [get_risk])\n",
    "# loss = loss[0,:,:]\n",
    "plot_risk_vs_return(scoring[0,:,:],scoring[1,:,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Max Risk\n",
    "batch = 1000\n",
    "iterations = 1000\n",
    "LR=1.0\n",
    "\n",
    "loss_fun = lambda x: npg.negative(get_risk_v(x))\n",
    "\n",
    "start_time = time.time()\n",
    "max_risk_weights, scoring = calculate_weights(len(ticks_filtered),\n",
    "                                                loss_fun, batch,\n",
    "                                                iterations, LR, \n",
    "                                                [get_risk_v, get_return_v])\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# least_risk_weights, loss = get_gd_weights_adam(len(ticks_filtered), get_risk, batch, iterations, LR, [get_risk])\n",
    "# loss = loss[0,:,:]\n",
    "plot_risk_vs_return(scoring[0,:,:],scoring[1,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Return Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Min Return\n",
    "batch = 1000\n",
    "iterations = 1000\n",
    "LR=1.0\n",
    "\n",
    "start_time = time.time()\n",
    "min_return_weights, scoring = calculate_weights(len(ticks_filtered),\n",
    "                                                 get_return_v, batch,\n",
    "                                                 iterations, LR,\n",
    "                                                 [get_risk_v,get_return_v])\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# most_return_weights, loss = get_gd_weights_adam(len(ticks_filtered), \n",
    "#                                            lambda x: -get_return(x),\n",
    "#                                            batch, iterations,\n",
    "#                                            LR, [get_risk])\n",
    "# loss = loss[0,:,:]\n",
    "plot_risk_vs_return(scoring[0,:,:],scoring[1,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get Max Return\n",
    "batch = 1000\n",
    "iterations = 1000\n",
    "LR=1.0\n",
    "loss_fun = lambda x: npg.negative(get_return_v(x))\n",
    "\n",
    "start_time = time.time()\n",
    "max_return_weights, scoring = calculate_weights(len(ticks_filtered),\n",
    "                                                 loss_fun, batch,\n",
    "                                                 iterations, LR,\n",
    "                                                 [get_risk_v,get_return_v])\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# most_return_weights, loss = get_gd_weights_adam(len(ticks_filtered), \n",
    "#                                            lambda x: -get_return(x),\n",
    "#                                            batch, iterations,\n",
    "#                                            LR, [get_risk])\n",
    "# loss = loss[0,:,:]\n",
    "plot_risk_vs_return(scoring[0,:,:],scoring[1,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get List of Max Returns for Spread of risks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_min = get_risk(min_risk_weights)\n",
    "risk_max = get_risk(max_risk_weights)\n",
    "risks_count = 50\n",
    "risks = np.linspace(risk_min, risk_max, risks_count, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 1000\n",
    "i = 1000\n",
    "lr = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Sharpe Spreads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch = [b]*risks_count\n",
    "iterations = [i]*risks_count\n",
    "LR=np.array([lr]*risks_count)\n",
    "# LR = LR*(1.0 + 50*np.abs(0.07 - risks))\n",
    "# LR[-3]*=10\n",
    "\n",
    "best_weights_range = np.random.uniform(size=(risks_count, len(ticks_filtered))) \n",
    "for r in range(risks_count):\n",
    "    start_time = time.time()\n",
    "    best_weights_range[r, :], scoring = calculate_weights(len(ticks_filtered), \n",
    "                                                                    get_loss_v(risk_target=risks[r]), \n",
    "                                                                    batch[r],\n",
    "                                                                    iterations[r],\n",
    "                                                                    LR[r],\n",
    "                                                                    [get_risk_v,get_return_v])\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    \n",
    "    plot_risk_vs_return(scoring[0,:,:],scoring[1,:,:], risk_title=\"Risk %f\"%risks[r])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min Sharpe Spreads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# batch = [b]*risks_count\n",
    "# iterations = [i]*risks_count\n",
    "# LR=np.array([lr]*risks_count)\n",
    "# # LR = LR*(1.0 + 50*np.abs(0.07 - risks))\n",
    "# # LR[-3]*=10\n",
    "\n",
    "# worst_weights_range = np.random.uniform(size=(risks_count, len(ticks_filtered))) \n",
    "# for r in range(risks_count):\n",
    "#     start_time = time.time()\n",
    "#     worst_weights_range[r, :], scoring = calculate_weights(len(ticks_filtered), \n",
    "#                                                                     get_loss_v(risk_target=risks[r], return_target=0.0), \n",
    "#                                                                     batch[r],\n",
    "#                                                                     iterations[r],\n",
    "#                                                                     LR[r],\n",
    "#                                                                     [get_risk_v,get_return_v])\n",
    "#     print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    \n",
    "#     plot_risk_vs_return(scoring[0,:,:],scoring[1,:,:], risk_title=\"Risk %f\"%risks[r])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profile to match inflation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch = 1000\n",
    "iterations = 1000\n",
    "LR=0.01\n",
    "\n",
    "start_time = time.time()\n",
    "inflation_weights, scoring = calculate_weights(len(ticks_filtered), \n",
    "                                           get_loss_v(return_target=inflation_rate),\n",
    "                                           batch, iterations,\n",
    "                                           LR, [get_risk_v,get_return_v])\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "plot_risk_vs_return(scoring[0,:,:],scoring[1,:,:], return_title=\"Return %f\"%inflation_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "b = 1000\n",
    "i = 1000\n",
    "lr = 0.015\n",
    "batch = [b]*len(profiles)\n",
    "iterations = [i]*len(profiles)\n",
    "LR=[lr]*len(profiles)\n",
    "\n",
    "# batch[2] = 10\n",
    "# iterations[2] = 200\n",
    "LR[2]=0.005\n",
    "target_weights = []\n",
    "\n",
    "for i in range(len(profiles)):\n",
    "    if profiles_targets[i] is not None:\n",
    "        wts = profile_makeup.loc[profiles[i]].to_numpy()[:-1]\n",
    "        print(i)\n",
    "        target_weights_, scoring = calculate_weights(len(ticks_filtered),\n",
    "                                                    profiles_targets[i],\n",
    "                                                    batch[i], iterations[i],\n",
    "                                                    LR[i], [get_risk_v,get_return_v],\n",
    "                                                    initial = filterTickers(wts, tick_allowed))\n",
    "        target_weights.append(target_weights_)\n",
    "#         risk,returns = scoring[0,:,:],scoring[1,:,:]\n",
    "        plot_risk_vs_return(scoring[0,:,:],scoring[1,:,:])\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        wts = profile_makeup.loc[profiles[i]].to_numpy()\n",
    "        target_weights.append(wts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAT = 0.5\n",
    "LUM = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_list = [colorsys.hls_to_rgb(h, LUM, SAT) for h in np.linspace(0.0, 1.0, len(ticks), endpoint=False)]\n",
    "color_list = [RGB(r*255,g*255,d*255) for r,g,d in color_list]\n",
    "color_list_accounts = [colorsys.hls_to_rgb(h, LUM, SAT) for h in np.linspace(0.0, 1.0, len(profiles), endpoint=False)]\n",
    "color_list_accounts = [RGB(r*255,g*255,d*255) for r,g,d in color_list_accounts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## PLOTS\n",
    "\n",
    "# ===== Setup Plot ====\n",
    "p = figure(\n",
    "    sizing_mode = \"stretch_both\", \n",
    "    title=\"Efficient frontier. Simulations: \" + str(num_profiles),\n",
    "    tools='box_zoom,wheel_zoom,reset', \n",
    "    toolbar_location='above',\n",
    ")\n",
    "p.add_tools(CrosshairTool(line_alpha=1, line_color='lightgray', line_width=1))\n",
    "p.add_tools(HoverTool(tooltips=None))\n",
    "\n",
    "p.xaxis.axis_label = 'Volatility, or risk (standard deviation)'\n",
    "p.yaxis.axis_label = 'Annual return'\n",
    "p.xaxis[0].formatter = NumeralTickFormatter(format=\"0.0%\")\n",
    "p.yaxis[0].formatter = NumeralTickFormatter(format=\"0.0%\")\n",
    "\n",
    "# ===== Render Funds ====\n",
    "wts = np.eye(len(ticks_filtered))\n",
    "risks = get_risk_v(wts)\n",
    "returns = get_return_v(wts)\n",
    "colors = filterTickers(color_list, tick_allowed)\n",
    "for i in range(len(ticks_filtered)):\n",
    "    c = p.circle(risks[i],returns[i],\n",
    "             color=colors[i],\n",
    "             legend_label=ticks_filtered[i], \n",
    "             size=10, alpha=0.8, )\n",
    "    p.add_tools(HoverTool(renderers=[c], tooltips=[\n",
    "        ('Tick', ticks_filtered[i])\n",
    "    ]))\n",
    "\n",
    "# ===== Render Boundries ====\n",
    "risk_boundry = Span(location=get_risk(min_risk_weights),\n",
    "                    dimension='height', line_color='green', line_width=1)\n",
    "return_boundry = Span(location=get_return(max_return_weights), \n",
    "                      dimension='width', line_color='green', line_width=1)\n",
    "inflation_boundry = Span(location=0.06, \n",
    "                      dimension='width', line_color='green', line_width=1)\n",
    "p.renderers.extend([risk_boundry, return_boundry, inflation_boundry])\n",
    "\n",
    "# ===== Render Best Sharpe Line ====\n",
    "boundry =np.concatenate([\n",
    "    np.reshape(min_risk_weights,(1, len(min_risk_weights))),\n",
    "    best_weights_range, \n",
    "    np.reshape(max_risk_weights,(1, len(max_risk_weights))),\n",
    "])\n",
    "\n",
    "c = p.circle(get_risk_v(boundry),\n",
    "         get_return_v(boundry),\n",
    "         color=\"teal\", legend_label=\"Max Sharpe Line\", size=4)\n",
    "l = p.line(get_risk_v(boundry),\n",
    "       get_return_v(boundry), \n",
    "       color=\"teal\",line_width=2)\n",
    "\n",
    "p.add_tools(HoverTool(renderers=[c, l], tooltips=[\n",
    "    ('Name', \"Max Sharpe Line\")\n",
    "]))\n",
    "\n",
    "# # ===== Render Worst Sharpe Line ====\n",
    "# boundry = np.concatenate([\n",
    "#     np.reshape(min_risk_weights,(1, len(min_risk_weights))),\n",
    "#     worst_weights_range, \n",
    "#     np.reshape(max_risk_weights,(1, len(max_risk_weights))),\n",
    "# ])\n",
    "\n",
    "# p.circle(get_risk_v(boundry),\n",
    "#          get_return_v(boundry),\n",
    "#          color=\"maroon\", legend_label=\"Max Sharpe Line\", size=4)\n",
    "# p.line(get_risk_v(boundry),\n",
    "#        get_return_v(boundry), \n",
    "#        color=\"maroon\", line_width=2)\n",
    "\n",
    "\n",
    "# ===== Render Sharpe Lines ====\n",
    "p.ray(x=[0],y=[0],length=0, angle=np.arctan(1.0), legend_label=\"Sharpe of 1.0\")\n",
    "p.ray(x=[0],y=[0],length=0, angle=np.arctan(2.0), legend_label=\"Sharpe of 2.0\")\n",
    "\n",
    "\n",
    "# ===== Render Existing Profile Pie Charts ====\n",
    "fidelity_pies = []\n",
    "for i in range(len(profiles)):\n",
    "    wts = get_weights(profile_makeup.loc[profiles[i]].to_numpy())\n",
    "#     print(\"wts\",wts)\n",
    "    fidelity_pies.append(\n",
    "        plot_portfolio_composition(ticks,\n",
    "                                   wts,\n",
    "                                   profiles_names[i],\n",
    "                                   color_list))\n",
    "    wts_filtered = filterTickers(wts, tick_allowed)\n",
    "    if np.sum(wts_filtered) != 0.0:\n",
    "        c = p.circle(get_risk(wts_filtered), get_return(wts_filtered), color=color_list_accounts[i], \n",
    "             legend_label=profiles_names[i], size=15)\n",
    "        p.add_tools(HoverTool(renderers=[c], tooltips=[\n",
    "            ('Profile', profiles_names[i])\n",
    "        ]))\n",
    "        \n",
    "# ===== Render Target Profile Pie Charts ====\n",
    "fidelity_targets = []\n",
    "for i in range(len(target_weights)):\n",
    "    fidelity_targets.append(\n",
    "        plot_portfolio_composition(\n",
    "            (ticks if len(target_weights[i]) > len(ticks_filtered) else ticks_filtered),\n",
    "            get_weights(target_weights[i]),\n",
    "            profiles_names[i] + \" Target\",\n",
    "            color_list\n",
    "        ))\n",
    "    if len(target_weights[i]) == len(ticks_filtered):\n",
    "        c = p.circle(get_risk(get_weights(target_weights[i])), get_return(get_weights(target_weights[i])),\n",
    "                 color=color_list_accounts[i], alpha=0.6,\n",
    "                 legend_label=profiles_names[i] + \" Target\", size=15)\n",
    "        p.add_tools(HoverTool(renderers=[c], tooltips=[\n",
    "            ('Profile', profiles_names[i] + \" Target\")\n",
    "        ]))\n",
    "        \n",
    "\n",
    "# ===== Adjusting Legend ====\n",
    "p.legend.location = \"top_left\"\n",
    "\n",
    "\n",
    "# ===== Create dashboard and open new window to show results ====\n",
    "layout_ = row([ \n",
    "        column([\n",
    "            p\n",
    "        ], sizing_mode = \"stretch_both\"),\n",
    "        column(fidelity_pies,\n",
    "               sizing_mode = \"stretch_height\"), \n",
    "        column(fidelity_targets\n",
    "               , sizing_mode = \"stretch_height\"),\n",
    "    ],width = 1500, sizing_mode = \"stretch_height\")\n",
    "    \n",
    "show(layout_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}